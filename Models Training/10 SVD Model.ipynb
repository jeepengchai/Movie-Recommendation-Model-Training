{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125dd0ff-069c-435f-84e6-69e4c3157ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Dataset for SVD: ./Datasets/ratings_preprocessed_D1.csv ---\n",
      "Loading data...\n",
      "Data loaded successfully. Shape: (100781, 27)\n",
      "Number of unique users (original IDs): 610\n",
      "Number of unique items (original IDs): 9686\n",
      "Splitting data (per user) into Train/Test...\n",
      "Users with items in test set: 610\n",
      "Surprise trainset built: 610 users, 8943 items.\n",
      "\n",
      "--- Training and Testing SVD: Dataset 1 ---\n",
      "Building and Training SVD model...\n",
      "SVD Training finished.\n",
      "\n",
      "Calculating SVD HR@10 and NDCG@10...\n",
      "Evaluating on 609 users present in trainset and test split.\n",
      "\n",
      "--- SVD Testing Summary ---\n",
      "Dataset: Dataset 1\n",
      "Tested HR/NDCG on 609 users.\n",
      "Hit Rate @10: 0.4122\n",
      "NDCG @10: 0.0845\n",
      "-----------------------------\n",
      "\n",
      "--- Processing Dataset for SVD: ./Datasets/ratings_preprocessed_D4.csv ---\n",
      "Loading data...\n",
      "Data loaded successfully. Shape: (1000209, 26)\n",
      "Number of unique users (original IDs): 6040\n",
      "Number of unique items (original IDs): 3706\n",
      "Splitting data (per user) into Train/Test...\n",
      "Users with items in test set: 6040\n",
      "Surprise trainset built: 6040 users, 3679 items.\n",
      "\n",
      "--- Training and Testing SVD: Dataset 2 ---\n",
      "Building and Training SVD model...\n",
      "SVD Training finished.\n",
      "\n",
      "Calculating SVD HR@10 and NDCG@10...\n",
      "Evaluating on 6039 users present in trainset and test split.\n",
      "\n",
      "--- SVD Testing Summary ---\n",
      "Dataset: Dataset 2\n",
      "Tested HR/NDCG on 6039 users.\n",
      "Hit Rate @10: 0.3721\n",
      "NDCG @10: 0.0611\n",
      "-----------------------------\n",
      "\n",
      "--- Plotting SVD Comparison (HR & NDCG Only) ---\n",
      "Saved comparison plot to: ./Training Results\\svd_comparison_metrics.png\n",
      "\n",
      "Comparison plotting complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# No LabelEncoder needed for this SVD approach\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os # For checking file paths\n",
    "import time # To time model training\n",
    "\n",
    "# Import Surprise library components\n",
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "# --- Configuration ---\n",
    "# --- Dataset 1 ---\n",
    "CSV_FILE_PATH_1 = './Datasets/ratings_preprocessed_D1.csv'\n",
    "DATASET_NAME_1 = \"Dataset 1\"\n",
    "\n",
    "# --- Dataset 2 ---\n",
    "CSV_FILE_PATH_2 = './Datasets/ratings_preprocessed_D4.csv'\n",
    "DATASET_NAME_2 = \"Dataset 2\"\n",
    "\n",
    "USER_COL = 'userId'\n",
    "ITEM_COL = 'movieId'\n",
    "RATING_COL_IMPLICIT = 'rating'\n",
    "\n",
    "# SVD (from Surprise) Model Hyperparameters\n",
    "SVD_N_FACTORS = 64 # Number of latent factors\n",
    "SVD_N_EPOCHS = 15  # Number of epochs for SVD training\n",
    "SVD_LR_ALL = 0.005 # Learning rate for SVD\n",
    "SVD_REG_ALL = 0.02  # Regularization term for SVD\n",
    "\n",
    "# Evaluation Parameters\n",
    "K = 10\n",
    "\n",
    "# --- File Saving Configuration ---\n",
    "SAVE_PLOTS = True\n",
    "PLOT_SAVE_DIR = \"./Training Results\"\n",
    "COMPARISON_PLOT_FILENAME = \"svd_comparison_metrics.png\"\n",
    "\n",
    "# Create plot save directory if it doesn't exist.\n",
    "if SAVE_PLOTS and not os.path.exists(PLOT_SAVE_DIR):\n",
    "    os.makedirs(PLOT_SAVE_DIR)\n",
    "    print(f\"Created directory for saving plots: {PLOT_SAVE_DIR}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_and_preprocess_for_svd(csv_path, user_col, item_col):\n",
    "    \"\"\"Loads data, preprocesses for SVD, and splits into train/test dicts.\"\"\"\n",
    "    print(f\"\\n--- Processing Dataset for SVD: {csv_path} ---\")\n",
    "    print(\"Loading data...\")\n",
    "    if not os.path.exists(csv_path):\n",
    "         print(f\"Error: CSV file not found at {csv_path}\")\n",
    "         return None\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "        df = df[[user_col, item_col]].copy().drop_duplicates()\n",
    "        df[RATING_COL_IMPLICIT] = 1.0 # Implicit feedback\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column {e} not found in {csv_path}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    n_users_orig = df[user_col].nunique()\n",
    "    n_items_orig = df[item_col].nunique()\n",
    "    print(f\"Number of unique users (original IDs): {n_users_orig}\")\n",
    "    print(f\"Number of unique items (original IDs): {n_items_orig}\")\n",
    "\n",
    "    # --- Train/Test Split (Per User) - No Validation Set ---\n",
    "    print(\"Splitting data (per user) into Train/Test...\")\n",
    "    train_interactions_dict = defaultdict(list) # user_id_orig -> [item_id_orig]\n",
    "    test_interactions_dict = defaultdict(list)  # user_id_orig -> [item_id_orig]\n",
    "    train_data_surprise_list = []               # list for Surprise: (user, item, rating)\n",
    "    users_in_test_set = set()\n",
    "\n",
    "    grouped = df.groupby(user_col)\n",
    "    for user_orig_id, group in grouped:\n",
    "        items_orig = list(group[item_col].unique())\n",
    "        if len(items_orig) < 2:\n",
    "            train_items_orig = items_orig\n",
    "            test_items_orig = []\n",
    "        else:\n",
    "            try: # Use same random_state for consistency if comparing splits\n",
    "                train_items_orig, test_items_orig = train_test_split(items_orig, test_size=0.2, random_state=42)\n",
    "            except ValueError: train_items_orig, test_items_orig = items_orig[:-1], items_orig[-1:]\n",
    "            if not test_items_orig and len(items_orig) > 0: train_items_orig, test_items_orig = items_orig[:-1], items_orig[-1:]\n",
    "\n",
    "        if train_items_orig:\n",
    "            train_interactions_dict[user_orig_id].extend(train_items_orig)\n",
    "            for item_id_orig in train_items_orig:\n",
    "                 train_data_surprise_list.append((user_orig_id, item_id_orig, 1.0))\n",
    "        if test_items_orig:\n",
    "            test_interactions_dict[user_orig_id].extend(test_items_orig)\n",
    "            users_in_test_set.add(user_orig_id)\n",
    "\n",
    "    users_in_test = list(users_in_test_set)\n",
    "    print(f\"Users with items in test set: {len(users_in_test)}\")\n",
    "\n",
    "    # --- Prepare Data for Surprise ---\n",
    "    if not train_data_surprise_list:\n",
    "        print(\"Error: No training data generated. Cannot proceed.\")\n",
    "        return None\n",
    "\n",
    "    train_df_surprise = pd.DataFrame(train_data_surprise_list, columns=[user_col, item_col, RATING_COL_IMPLICIT])\n",
    "    reader = Reader(rating_scale=(1, 1))\n",
    "    try:\n",
    "        surprise_dataset = Dataset.load_from_df(train_df_surprise[[user_col, item_col, RATING_COL_IMPLICIT]], reader)\n",
    "        trainset = surprise_dataset.build_full_trainset() # Build trainset ONLY from train interactions\n",
    "        print(f\"Surprise trainset built: {trainset.n_users} users, {trainset.n_items} items.\")\n",
    "        all_raw_item_ids_list = df[item_col].unique().tolist() # Get all original item IDs\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Surprise dataset or trainset: {e}\")\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"n_users_orig\": n_users_orig, \"n_items_orig\": n_items_orig,\n",
    "        \"trainset_surprise\": trainset,\n",
    "        \"train_interactions_dict\": train_interactions_dict,\n",
    "        \"test_interactions_dict\": test_interactions_dict,\n",
    "        \"users_in_test\": users_in_test,\n",
    "        \"all_raw_item_ids\": all_raw_item_ids_list\n",
    "    }\n",
    "\n",
    "# --- SVD Recommendation Function ---\n",
    "def get_recommendations_svd(user_orig_id, trainset, svd_model, k, all_raw_item_ids):\n",
    "    \"\"\"Gets top K recommendations for a user using Surprise SVD model.\"\"\"\n",
    "    try: trainset.to_inner_uid(user_orig_id) # Check if user is known\n",
    "    except ValueError: return [] # User not in trainset\n",
    "\n",
    "    predictions = []\n",
    "    user_train_items = set()\n",
    "    try: # Get items user interacted with in training\n",
    "        user_inner_id = trainset.to_inner_uid(user_orig_id)\n",
    "        user_train_items = set(trainset.to_raw_iid(inner_iid) for inner_iid, _ in trainset.ur[user_inner_id])\n",
    "    except ValueError: pass\n",
    "\n",
    "    for item_raw_id in all_raw_item_ids:\n",
    "        if item_raw_id not in user_train_items:\n",
    "             pred = svd_model.predict(uid=user_orig_id, iid=item_raw_id)\n",
    "             if not pred.details.get('was_impossible', False):\n",
    "                 predictions.append((item_raw_id, pred.est))\n",
    "\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_k_raw_ids = [item_raw_id for item_raw_id, score in predictions[:k]]\n",
    "    return top_k_raw_ids\n",
    "\n",
    "# --- Common Metric Calculation Function ---\n",
    "def calculate_metrics(recommendations_dict, test_interactions_dict, users_to_eval, k):\n",
    "    \"\"\"Calculates HR@K and NDCG@K.\"\"\"\n",
    "    hits, total_ndcg, evaluated_user_count = 0, 0.0, 0\n",
    "    for user_id in users_to_eval: # Original ID\n",
    "        if user_id not in test_interactions_dict or not test_interactions_dict[user_id]: continue\n",
    "        if user_id not in recommendations_dict: continue\n",
    "\n",
    "        evaluated_user_count += 1\n",
    "        recommended_indices = recommendations_dict[user_id] # Original item IDs\n",
    "        test_items_set = set(test_interactions_dict[user_id]) # Original item IDs\n",
    "\n",
    "        hit = any(item_id in test_items_set for item_id in recommended_indices)\n",
    "        if hit: hits += 1\n",
    "\n",
    "        dcg = sum(1.0 / math.log2(i + 2) for i, item_id in enumerate(recommended_indices) if item_id in test_items_set)\n",
    "        idcg = sum(1.0 / math.log2(i + 2) for i in range(min(k, len(test_items_set))))\n",
    "        total_ndcg += (dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "    avg_hit_rate = hits / evaluated_user_count if evaluated_user_count > 0 else 0.0\n",
    "    avg_ndcg = total_ndcg / evaluated_user_count if evaluated_user_count > 0 else 0.0\n",
    "    return avg_hit_rate, avg_ndcg, evaluated_user_count\n",
    "\n",
    "# --- Training and Evaluation Function for SVD ---\n",
    "def train_evaluate_svd(dataset_name, data_dict, model_params, eval_params):\n",
    "    \"\"\"Trains and tests the SVD model.\"\"\"\n",
    "    print(f\"\\n--- Training and Testing SVD: {dataset_name} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    trainset = data_dict['trainset_surprise']\n",
    "    users_in_test = data_dict['users_in_test'] # Original user IDs\n",
    "    test_interactions_dict = data_dict['test_interactions_dict'] # Original IDs\n",
    "    all_raw_item_ids = data_dict['all_raw_item_ids']\n",
    "\n",
    "    k = eval_params['k']\n",
    "    n_factors = model_params['svd_n_factors']\n",
    "    n_epochs = model_params['svd_n_epochs']\n",
    "    lr_all = model_params['svd_lr_all']\n",
    "    reg_all = model_params['svd_reg_all']\n",
    "\n",
    "    # Build and Train Model (No validation set used during fit)\n",
    "    print(\"Building and Training SVD model...\")\n",
    "    svd_model = SVD(n_factors=n_factors, n_epochs=n_epochs, lr_all=lr_all, reg_all=reg_all, random_state=42, verbose=False)\n",
    "    svd_model.fit(trainset) # Train on the prepared trainset\n",
    "    print(\"SVD Training finished.\")\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Evaluate HR@K, NDCG@K on Test Set\n",
    "    print(f\"\\nCalculating SVD HR@{k} and NDCG@{k}...\")\n",
    "    svd_recs = {}\n",
    "    valid_test_users_for_eval = [uid for uid in users_in_test\n",
    "                                 if uid in test_interactions_dict and test_interactions_dict[uid]\n",
    "                                 and trainset.knows_user(uid)] # Ensure user was in trainset\n",
    "    print(f\"Evaluating on {len(valid_test_users_for_eval)} users present in trainset and test split.\")\n",
    "\n",
    "    eval_start_time = time.time()\n",
    "    for user_id in valid_test_users_for_eval: # user_id is original ID\n",
    "         svd_recs[user_id] = get_recommendations_svd(user_id, trainset, svd_model, k, all_raw_item_ids)\n",
    "    eval_time = time.time() - eval_start_time\n",
    "\n",
    "    hit_rate, ndcg, eval_count = calculate_metrics(svd_recs, test_interactions_dict, valid_test_users_for_eval, k)\n",
    "\n",
    "    print(\"\\n--- SVD Testing Summary ---\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    #print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    #print(f\"Evaluation Time (Rec Generation): {eval_time:.2f} seconds\")\n",
    "    print(f\"Tested HR/NDCG on {eval_count} users.\")\n",
    "    print(f\"Hit Rate @{k}: {hit_rate:.4f}\")\n",
    "    print(f\"NDCG @{k}: {ndcg:.4f}\")\n",
    "    print(\"-----------------------------\")\n",
    "\n",
    "    # Return metrics needed for plotting (HR, NDCG) - Training time removed from return as not plotted\n",
    "    return {'HR@K': hit_rate, 'NDCG@K': ndcg, 'TrainTime': training_time} # Keep TrainTime for filtering results if needed\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "results = {}\n",
    "dataset_names = [DATASET_NAME_1, DATASET_NAME_2]\n",
    "csv_paths = [CSV_FILE_PATH_1, CSV_FILE_PATH_2]\n",
    "\n",
    "model_params = { # SVD params\n",
    "    'svd_n_factors': SVD_N_FACTORS, 'svd_n_epochs': SVD_N_EPOCHS,\n",
    "    'svd_lr_all': SVD_LR_ALL, 'svd_reg_all': SVD_REG_ALL\n",
    "}\n",
    "eval_params = {'k': K}\n",
    "\n",
    "for name, path in zip(dataset_names, csv_paths):\n",
    "    data = load_and_preprocess_for_svd(path, USER_COL, ITEM_COL)\n",
    "    if data:\n",
    "        svd_results = train_evaluate_svd(name, data, model_params, eval_params)\n",
    "        results[name] = svd_results\n",
    "    else:\n",
    "        # Ensure default dict has keys expected later, even if values are default\n",
    "        results[name] = {'HR@K': 0.0, 'NDCG@K': 0.0, 'TrainTime': -1.0} # Use -1 for time to indicate failure\n",
    "\n",
    "\n",
    "# --- Plot Comparison ---\n",
    "print(\"\\n--- Plotting SVD Comparison (HR & NDCG Only) ---\")\n",
    "\n",
    "if not results:\n",
    "    print(\"No results available to plot.\")\n",
    "else:\n",
    "    # Filter ensures datasets processed successfully (TrainTime >= 0)\n",
    "    valid_dataset_names = [name for name in dataset_names if name in results and results[name]['TrainTime'] >= 0]\n",
    "    if not valid_dataset_names:\n",
    "        print(\"No valid results available to plot.\")\n",
    "    else:\n",
    "        # Metrics to plot: HR@K and NDCG@K only\n",
    "        metrics_to_plot = ['HR@K', 'NDCG@K']\n",
    "        metric_titles = {\n",
    "            'HR@K': f'Hit Rate @{K}',\n",
    "            'NDCG@K': f'NDCG @{K}',\n",
    "            # 'TrainTime': 'Training Time (seconds)' # Removed\n",
    "            # 'TrainLoss': 'Training Loss' # Cannot plot training loss curve for SVD\n",
    "        }\n",
    "\n",
    "        n_datasets = len(valid_dataset_names)\n",
    "        x = np.arange(n_datasets)\n",
    "        width = 0.5 # Width for single bars per dataset\n",
    "\n",
    "        num_metrics = len(metrics_to_plot) # Should be 2\n",
    "        fig_comp, axes = plt.subplots(1, num_metrics, figsize=(6 * num_metrics, 5)) # Creates 1 row, 2 columns\n",
    "        if num_metrics == 1: axes = [axes] # Make iterable (good practice, though should be 2)\n",
    "\n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[i]\n",
    "            metric_values = [results[d_name].get(metric, 0) for d_name in valid_dataset_names]\n",
    "            bars = ax.bar(x, metric_values, width)\n",
    "            fmt_str = '%.4f' # Format for HR and NDCG\n",
    "            ax.bar_label(bars, padding=3, fmt=fmt_str)\n",
    "\n",
    "            ax.set_ylabel(metric_titles[metric])\n",
    "            ax.set_title(f'SVD {metric_titles[metric]} Comparison')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(valid_dataset_names)\n",
    "            ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "            max_val = max(metric_values) if metric_values else 0\n",
    "            ax.set_ylim(0, max(max_val * 1.2, 0.02)) # Dynamic y-axis with padding\n",
    "\n",
    "        fig_comp.tight_layout() # Adjust layout\n",
    "\n",
    "        # Save or Show the plot\n",
    "        if SAVE_PLOTS:\n",
    "            save_path = os.path.join(PLOT_SAVE_DIR, COMPARISON_PLOT_FILENAME)\n",
    "            try:\n",
    "                fig_comp.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "                print(f\"Saved comparison plot to: {save_path}\")\n",
    "                plt.close(fig_comp) # Close the figure after saving\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving comparison plot: {e}\")\n",
    "                plt.show() # Show if saving failed\n",
    "        else:\n",
    "            plt.show() # Show the plot if not saving\n",
    "\n",
    "print(\"\\nComparison plotting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc4895-8e6b-42b6-bf59-94578bf03a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
