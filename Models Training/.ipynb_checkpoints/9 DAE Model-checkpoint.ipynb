{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a640044-1e48-4f72-a152-e7357746a057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Dataset for DAE: ./Datasets/ratings_preprocessed_D1.csv ---\n",
      "Loading data...\n",
      "Data loaded successfully. Shape: (100781, 27)\n",
      "Preprocessing data (Label Encoding)...\n",
      "Number of unique users (encoded): 610\n",
      "Number of unique items (encoded): 9686\n",
      "Splitting data (per user)...\n",
      "Users with items in test set: 610\n",
      "\n",
      "--- Training and Testing DAE: Dataset 1 ---\n",
      "Building DAE model...\n",
      "Training DAE model...\n",
      "Epoch 1/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 0.6329\n",
      "Epoch 2/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - loss: 0.5072\n",
      "Epoch 3/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - loss: 0.2784\n",
      "Epoch 4/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - loss: 0.0972\n",
      "Epoch 5/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - loss: 2.3416\n",
      "Epoch 6/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - loss: 0.0982\n",
      "Epoch 7/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - loss: 0.0817\n",
      "Epoch 8/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - loss: 0.0731\n",
      "Epoch 9/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - loss: 0.0697\n",
      "Epoch 10/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - loss: 0.0634\n",
      "Epoch 11/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - loss: 0.0619\n",
      "Epoch 12/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - loss: 0.0577\n",
      "Epoch 13/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - loss: 0.0612\n",
      "Epoch 14/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - loss: 0.0534\n",
      "Epoch 15/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - loss: 0.0540\n",
      "DAE Training finished. Time: 20.65s\n",
      "Saved DAE loss plot to: ./Training Results\\dae_loss_curve_Dataset_1.png\n",
      "\n",
      "Testing DAE model...\n",
      "Final Training Loss: 0.0545\n",
      "Test Loss: 0.0479\n",
      "\n",
      "Calculating DAE HR@10 and NDCG@10...\n",
      "Testing HR/NDCG on 610 users.\n",
      "\n",
      "--- DAE Test Summary ---\n",
      "Dataset: Dataset 1\n",
      "Final Training Loss: 0.0545\n",
      "Test Loss: 0.0479\n",
      "Evaluated HR/NDCG on 610 users.\n",
      "Hit Rate @10: 0.5967\n",
      "NDCG @10: 0.1504\n",
      "-----------------------------\n",
      "\n",
      "--- Processing Dataset for DAE: ./Datasets/ratings_preprocessed_D4.csv ---\n",
      "Loading data...\n",
      "Data loaded successfully. Shape: (1000209, 26)\n",
      "Preprocessing data (Label Encoding)...\n",
      "Number of unique users (encoded): 6040\n",
      "Number of unique items (encoded): 3706\n",
      "Splitting data (per user)...\n",
      "Users with items in test set: 6040\n",
      "\n",
      "--- Training and Testing DAE: Dataset 2 ---\n",
      "Building DAE model...\n",
      "Training DAE model...\n",
      "Epoch 1/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - loss: 0.5551\n",
      "Epoch 2/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.1213\n",
      "Epoch 3/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.1219\n",
      "Epoch 4/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.1204\n",
      "Epoch 5/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.1220\n",
      "Epoch 6/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.1236\n",
      "Epoch 7/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.1224\n",
      "Epoch 8/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.1193\n",
      "Epoch 9/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 0.1200\n",
      "Epoch 10/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.1173\n",
      "Epoch 11/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.1167\n",
      "Epoch 12/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.6167\n",
      "Epoch 13/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.8666\n",
      "Epoch 14/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 0.1769\n",
      "Epoch 15/15\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.1689\n",
      "DAE Training finished. Time: 63.88s\n",
      "Saved DAE loss plot to: ./Training Results\\dae_loss_curve_Dataset_2.png\n",
      "\n",
      "Testing DAE model...\n",
      "Final Training Loss: 0.1695\n",
      "Test Loss: 0.1561\n",
      "\n",
      "Calculating DAE HR@10 and NDCG@10...\n",
      "Testing HR/NDCG on 6040 users.\n",
      "\n",
      "--- DAE Test Summary ---\n",
      "Dataset: Dataset 2\n",
      "Final Training Loss: 0.1695\n",
      "Test Loss: 0.1561\n",
      "Evaluated HR/NDCG on 6040 users.\n",
      "Hit Rate @10: 0.4010\n",
      "NDCG @10: 0.0721\n",
      "-----------------------------\n",
      "\n",
      "--- Plotting DAE Comparison (HR & NDCG Only) ---\n",
      "Saved comparison plot (HR & NDCG) to: ./Training Results\\dae_comparison_metrics.png\n",
      "\n",
      "Comparison plotting complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os # For checking file paths\n",
    "import time # To time model training\n",
    "\n",
    "# --- Configuration ---\n",
    "# --- Dataset 1 ---\n",
    "CSV_FILE_PATH_1 = './Datasets/ratings_preprocessed_D1.csv'\n",
    "DATASET_NAME_1 = \"Dataset 1\"\n",
    "\n",
    "# --- Dataset 2 ---\n",
    "CSV_FILE_PATH_2 = './Datasets/ratings_preprocessed_D4.csv'\n",
    "DATASET_NAME_2 = \"Dataset 2\"\n",
    "\n",
    "USER_COL = 'userId'\n",
    "ITEM_COL = 'movieId'\n",
    "\n",
    "# Model Hyperparameters\n",
    "ENCODING_DIM = 64\n",
    "DENSE_LAYERS = [512, 256]\n",
    "ACTIVATION = 'selu'\n",
    "OUT_ACTIVATION = 'sigmoid'\n",
    "DROPOUT_RATE = 0.1\n",
    "LEARNING_RATE = 0.01 # Adjusted, check convergence\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Evaluation Parameters\n",
    "K = 10\n",
    "\n",
    "# --- File Saving Configuration ---\n",
    "SAVE_PLOTS = True\n",
    "PLOT_SAVE_DIR = \"./Training Results\"\n",
    "COMPARISON_PLOT_FILENAME = \"dae_comparison_metrics.png\"\n",
    "INDIVIDUAL_LOSS_PLOT_PREFIX = \"dae_loss_curve\"\n",
    "\n",
    "# Create plot save directory if it doesn't exist\n",
    "if SAVE_PLOTS and not os.path.exists(PLOT_SAVE_DIR):\n",
    "    os.makedirs(PLOT_SAVE_DIR)\n",
    "    print(f\"Created directory for saving plots: {PLOT_SAVE_DIR}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_and_preprocess_for_dae(csv_path, user_col, item_col):\n",
    "    \"\"\"Loads data, preprocesses, and splits into train/test for DAE.\"\"\"\n",
    "    print(f\"\\n--- Processing Dataset for DAE: {csv_path} ---\")\n",
    "    print(\"Loading data...\")\n",
    "    if not os.path.exists(csv_path):\n",
    "         print(f\"Error: CSV file not found at {csv_path}\")\n",
    "         return None\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "        df = df[[user_col, item_col]].copy().drop_duplicates()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column {e} not found in {csv_path}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"Preprocessing data (Label Encoding)...\")\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "    df['user_id_encoded'] = user_encoder.fit_transform(df[user_col])\n",
    "    df['item_id_encoded'] = item_encoder.fit_transform(df[item_col])\n",
    "\n",
    "    n_users = df['user_id_encoded'].nunique()\n",
    "    n_items = df['item_id_encoded'].nunique()\n",
    "    print(f\"Number of unique users (encoded): {n_users}\")\n",
    "    print(f\"Number of unique items (encoded): {n_items}\")\n",
    "\n",
    "    user_item_interactions = df.groupby('user_id_encoded')['item_id_encoded'].apply(list).to_dict()\n",
    "\n",
    "    print(\"Splitting data (per user)...\")\n",
    "    train_interactions = defaultdict(list) # Encoded {user: [items]} for masking\n",
    "    test_interactions = defaultdict(list)  # Encoded {user: [items]} for ground truth\n",
    "    user_vectors_train = np.zeros((n_users, n_items), dtype=np.float32)\n",
    "    user_vectors_test_input = np.zeros((n_users, n_items), dtype=np.float32)\n",
    "    users_in_test = [] # List of encoded user IDs w/ test items\n",
    "\n",
    "    for user_id, items in user_item_interactions.items():\n",
    "        if len(items) < 2:\n",
    "            train_items = items\n",
    "            test_items = []\n",
    "            if train_items:\n",
    "                for item_id in train_items:\n",
    "                    if 0 <= user_id < n_users and 0 <= item_id < n_items:\n",
    "                        train_interactions[user_id].append(item_id)\n",
    "                        user_vectors_train[user_id, item_id] = 1.0\n",
    "                        user_vectors_test_input[user_id, item_id] = 1.0\n",
    "        else:\n",
    "            try: train_items, test_items = train_test_split(items, test_size=0.2, random_state=42)\n",
    "            except ValueError: train_items, test_items = items[:-1], items[-1:]\n",
    "            if not test_items and len(items) > 0: train_items, test_items = items[:-1], items[-1:]\n",
    "\n",
    "            if train_items:\n",
    "                for item_id in train_items:\n",
    "                     if 0 <= user_id < n_users and 0 <= item_id < n_items:\n",
    "                         train_interactions[user_id].append(item_id)\n",
    "                         user_vectors_train[user_id, item_id] = 1.0\n",
    "                         user_vectors_test_input[user_id, item_id] = 1.0\n",
    "            if test_items:\n",
    "                valid_test_item_added = False\n",
    "                for item_id in test_items:\n",
    "                     if 0 <= item_id < n_items:\n",
    "                         test_interactions[user_id].append(item_id)\n",
    "                         valid_test_item_added = True\n",
    "                if valid_test_item_added and 0 <= user_id < n_users:\n",
    "                    users_in_test.append(user_id)\n",
    "\n",
    "    print(f\"Users with items in test set: {len(users_in_test)}\")\n",
    "    user_vectors_train_target = user_vectors_train.copy()\n",
    "\n",
    "    return {\n",
    "        \"n_users\": n_users, \"n_items\": n_items,\n",
    "        \"user_encoder\": user_encoder, \"item_encoder\": item_encoder,\n",
    "        \"user_vectors_train\": user_vectors_train,\n",
    "        \"user_vectors_train_target\": user_vectors_train_target,\n",
    "        \"user_vectors_test_input\": user_vectors_test_input,\n",
    "        \"train_interactions\": train_interactions,\n",
    "        \"test_interactions\": test_interactions,\n",
    "        \"users_in_test\": users_in_test,\n",
    "    }\n",
    "\n",
    "def build_autoencoder(n_items, encoding_dim, dense_layers, activation, out_activation, dropout_rate):\n",
    "    \"\"\"Builds the Keras Autoencoder model.\"\"\"\n",
    "    input_layer = keras.Input(shape=(n_items,))\n",
    "    x = input_layer; x = layers.Dropout(dropout_rate)(x)\n",
    "    for neurons in dense_layers: x = layers.Dense(neurons, activation=activation)(x); x = layers.Dropout(dropout_rate)(x)\n",
    "    encoded = layers.Dense(encoding_dim, activation=activation, name='embedding')(x)\n",
    "    x = encoded\n",
    "    for neurons in reversed(dense_layers): x = layers.Dense(neurons, activation=activation)(x); x = layers.Dropout(dropout_rate)(x)\n",
    "    decoded = layers.Dense(n_items, activation=out_activation)(x)\n",
    "    autoencoder = keras.Model(input_layer, decoded, name='Autoencoder')\n",
    "    return autoencoder\n",
    "\n",
    "def get_recommendations_dae(user_id_encoded, user_vector_input, model, train_items_set, k):\n",
    "    \"\"\"Generates top-K recommendations using the DAE model.\"\"\"\n",
    "    scores = model.predict(np.expand_dims(user_vector_input, axis=0), verbose=0)[0]\n",
    "    valid_train_indices = [idx for idx in train_items_set if idx < len(scores)]\n",
    "    scores[valid_train_indices] = -np.inf # Mask seen items\n",
    "    top_k_indices = np.argsort(scores)[::-1]\n",
    "    n_items_output = model.output_shape[1]\n",
    "    valid_top_k = [idx for idx in top_k_indices if idx < n_items_output][:k]\n",
    "    return valid_top_k\n",
    "\n",
    "def calculate_metrics(recommendations_dict, test_interactions_dict, users_to_eval, k):\n",
    "    \"\"\"Calculates HR@K and NDCG@K.\"\"\"\n",
    "    hits, total_ndcg, evaluated_user_count = 0, 0.0, 0\n",
    "    for user_id in users_to_eval: # Encoded ID\n",
    "        if user_id not in test_interactions_dict or not test_interactions_dict[user_id]: continue\n",
    "        if user_id not in recommendations_dict: continue\n",
    "\n",
    "        evaluated_user_count += 1\n",
    "        recommended_indices = recommendations_dict[user_id] # Encoded item IDs\n",
    "        test_items_set = set(test_interactions_dict[user_id]) # Encoded item IDs\n",
    "\n",
    "        hit = any(item_id in test_items_set for item_id in recommended_indices)\n",
    "        if hit: hits += 1\n",
    "\n",
    "        dcg = sum(1.0 / math.log2(i + 2) for i, item_id in enumerate(recommended_indices) if item_id in test_items_set)\n",
    "        idcg = sum(1.0 / math.log2(i + 2) for i in range(min(k, len(test_items_set))))\n",
    "        total_ndcg += (dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "    avg_hit_rate = hits / evaluated_user_count if evaluated_user_count > 0 else 0.0\n",
    "    avg_ndcg = total_ndcg / evaluated_user_count if evaluated_user_count > 0 else 0.0\n",
    "    return avg_hit_rate, avg_ndcg, evaluated_user_count\n",
    "\n",
    "# --- Training and Evaluation Function for DAE (No Validation during Fit) ---\n",
    "def train_evaluate_dae(dataset_name, data_dict, model_params, eval_params, save_plots_config):\n",
    "    \"\"\"Trains DAE, saves/shows loss curve, evaluates HR/NDCG.\"\"\"\n",
    "    print(f\"\\n--- Training and Testing DAE: {dataset_name} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    n_items = data_dict['n_items']\n",
    "    user_vectors_train = data_dict['user_vectors_train']\n",
    "    user_vectors_train_target = data_dict['user_vectors_train_target']\n",
    "    users_in_test = data_dict['users_in_test']\n",
    "    user_vectors_test_input = data_dict['user_vectors_test_input']\n",
    "    train_interactions = data_dict['train_interactions']\n",
    "    test_interactions = data_dict['test_interactions']\n",
    "\n",
    "    k = eval_params['k']\n",
    "    epochs = model_params['epochs']\n",
    "    batch_size = model_params['batch_size']\n",
    "    learning_rate = model_params['learning_rate']\n",
    "\n",
    "    print(\"Building DAE model...\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_autoencoder(\n",
    "        n_items, model_params['encoding_dim'], model_params['dense_layers'],\n",
    "        model_params['activation'], model_params['out_activation'], model_params['dropout_rate']\n",
    "    )\n",
    "\n",
    "    print(\"Training DAE model...\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    callbacks = []\n",
    "\n",
    "    history = model.fit(\n",
    "        user_vectors_train, user_vectors_train_target,\n",
    "        epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "        callbacks=callbacks, verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"DAE Training finished. Time: {training_time:.2f}s\")\n",
    "\n",
    "    # --- Plot and Save/Show INDIVIDUAL Training Loss Curve ---\n",
    "    fig_loss, ax_loss = plt.subplots(figsize=(10, 5))\n",
    "    ax_loss.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    ax_loss.set_title(f'DAE Model Training Loss - {dataset_name}')\n",
    "    ax_loss.set_ylabel('Loss (Binary Crossentropy)')\n",
    "    ax_loss.set_xlabel('Epoch')\n",
    "    ax_loss.legend(loc='best'); ax_loss.grid(True)\n",
    "\n",
    "    if save_plots_config['save']:\n",
    "        safe_name = \"\".join(c if c.isalnum() else \"_\" for c in dataset_name)\n",
    "        f_path = os.path.join(save_plots_config['dir'], f\"{save_plots_config['prefix']}_{safe_name}.png\")\n",
    "        try:\n",
    "            fig_loss.savefig(f_path, bbox_inches='tight', dpi=150)\n",
    "            print(f\"Saved DAE loss plot to: {f_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving DAE loss plot: {e}\")\n",
    "        finally:\n",
    "             plt.close(fig_loss) # Close the figure regardless of save success/failure\n",
    "    else:\n",
    "        plt.show() # Show plot if not saving\n",
    "\n",
    "    # --- Test Model ---\n",
    "    print(\"\\nTesting DAE model...\")\n",
    "    train_loss = history.history['loss'][-1] if 'loss' in history.history and history.history['loss'] else float('nan')\n",
    "\n",
    "    # Calculate reconstruction loss on the training set\n",
    "    test_loss_recon_train = \"N/A\"\n",
    "    if user_vectors_train.shape[0] > 0:\n",
    "        try: loss_val = model.evaluate(user_vectors_train, user_vectors_train_target, batch_size=batch_size, verbose=0); test_loss_recon_train = f\"{loss_val:.4f}\"\n",
    "        except Exception as e: print(f\"Error evaluating recon loss: {e}\"); test_loss_recon_train = \"Error\"\n",
    "    else: test_loss_recon_train = \"N/A\"\n",
    "\n",
    "    print(f\"Final Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss_recon_train}\")\n",
    "\n",
    "    # --- Evaluate HR@K, NDCG@K ---\n",
    "    print(f\"\\nCalculating DAE HR@{k} and NDCG@{k}...\")\n",
    "    dae_recs = {}\n",
    "    valid_test_users_for_eval = [uid for uid in users_in_test\n",
    "                                 if uid in test_interactions and test_interactions[uid]\n",
    "                                 and uid < user_vectors_test_input.shape[0]]\n",
    "    print(f\"Testing HR/NDCG on {len(valid_test_users_for_eval)} users.\")\n",
    "\n",
    "    eval_start_time = time.time()\n",
    "    for user_id in valid_test_users_for_eval: # Encoded ID\n",
    "        train_items_set = set(train_interactions.get(user_id, []))\n",
    "        user_vector_input = user_vectors_test_input[user_id]\n",
    "        dae_recs[user_id] = get_recommendations_dae(user_id, user_vector_input, model, train_items_set, k)\n",
    "    eval_time = time.time() - eval_start_time\n",
    "\n",
    "    hit_rate, ndcg, eval_count = calculate_metrics(dae_recs, test_interactions, valid_test_users_for_eval, k)\n",
    "\n",
    "    print(\"\\n--- DAE Test Summary ---\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    #print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    #print(f\"Evaluation Time (Rec Gen): {eval_time:.2f} seconds\")\n",
    "    print(f\"Final Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss_recon_train}\")\n",
    "    print(f\"Evaluated HR/NDCG on {eval_count} users.\")\n",
    "    print(f\"Hit Rate @{k}: {hit_rate:.4f}\")\n",
    "    print(f\"NDCG @{k}: {ndcg:.4f}\")\n",
    "    print(\"-----------------------------\")\n",
    "\n",
    "    # Return only HR and NDCG for the comparison plot\n",
    "    return {'HR@K': hit_rate, 'NDCG@K': ndcg}\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "results = {}\n",
    "dataset_names = [DATASET_NAME_1, DATASET_NAME_2]\n",
    "csv_paths = [CSV_FILE_PATH_1, CSV_FILE_PATH_2]\n",
    "\n",
    "model_params = { # DAE params\n",
    "    'encoding_dim': ENCODING_DIM, 'dense_layers': DENSE_LAYERS,\n",
    "    'activation': ACTIVATION, 'out_activation': OUT_ACTIVATION,\n",
    "    'dropout_rate': DROPOUT_RATE, 'learning_rate': LEARNING_RATE,\n",
    "    'epochs': EPOCHS, 'batch_size': BATCH_SIZE\n",
    "}\n",
    "eval_params = {'k': K}\n",
    "save_plots_config = {\n",
    "    'save': SAVE_PLOTS,\n",
    "    'dir': PLOT_SAVE_DIR,\n",
    "    'prefix': INDIVIDUAL_LOSS_PLOT_PREFIX\n",
    "}\n",
    "\n",
    "for name, path in zip(dataset_names, csv_paths):\n",
    "    data = load_and_preprocess_for_dae(path, USER_COL, ITEM_COL)\n",
    "    if data:\n",
    "        dae_results = train_evaluate_dae(name, data, model_params, eval_params, save_plots_config)\n",
    "        results[name] = dae_results # Store only HR and NDCG\n",
    "    else:\n",
    "        results[name] = {'HR@K': 0.0, 'NDCG@K': 0.0} # Default results\n",
    "\n",
    "# --- Plot Comparison (HR and NDCG Only) ---\n",
    "print(\"\\n--- Plotting DAE Comparison (HR & NDCG Only) ---\")\n",
    "\n",
    "if not results:\n",
    "    print(\"No results available to plot.\")\n",
    "else:\n",
    "    # Check if results were successfully generated for datasets\n",
    "    valid_dataset_names = [name for name in dataset_names if name in results and 'HR@K' in results[name]]\n",
    "    if not valid_dataset_names:\n",
    "        print(\"No valid results available to plot.\")\n",
    "    else:\n",
    "        # Metrics to plot: HR@K and NDCG@K only\n",
    "        metrics_to_plot = ['HR@K', 'NDCG@K']\n",
    "        metric_titles = {\n",
    "            'HR@K': f'Hit Rate @{K}',\n",
    "            'NDCG@K': f'NDCG @{K}'\n",
    "            # Removed TrainTime and TrainLoss titles\n",
    "        }\n",
    "\n",
    "        n_datasets = len(valid_dataset_names)\n",
    "        x = np.arange(n_datasets)\n",
    "        width = 0.5 # Width for single bars per dataset\n",
    "\n",
    "        num_metrics = len(metrics_to_plot) # Should be 2\n",
    "        fig_comp, axes = plt.subplots(1, num_metrics, figsize=(6 * num_metrics, 5))\n",
    "        if num_metrics == 1: axes = [axes] # Make iterable\n",
    "\n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[i]\n",
    "            metric_values = [results[d_name].get(metric, 0) for d_name in valid_dataset_names]\n",
    "            bars = ax.bar(x, metric_values, width)\n",
    "            fmt_str = '%.4f' # Format for HR and NDCG\n",
    "            ax.bar_label(bars, padding=3, fmt=fmt_str)\n",
    "\n",
    "            ax.set_ylabel(metric_titles[metric])\n",
    "            ax.set_title(f'DAE {metric_titles[metric]} Comparison')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(valid_dataset_names)\n",
    "            ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "            max_val = max(metric_values) if metric_values else 0\n",
    "            ax.set_ylim(0, max(max_val * 1.2, 0.02)) # Dynamic y-axis with padding\n",
    "\n",
    "        fig_comp.tight_layout() # Adjust layout\n",
    "\n",
    "        # Save or Show the plot\n",
    "        if SAVE_PLOTS:\n",
    "            save_path = os.path.join(PLOT_SAVE_DIR, COMPARISON_PLOT_FILENAME)\n",
    "            try:\n",
    "                fig_comp.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "                print(f\"Saved comparison plot (HR & NDCG) to: {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving comparison plot: {e}\")\n",
    "            finally:\n",
    "                 plt.close(fig_comp) # Close the figure after attempting save\n",
    "        else:\n",
    "            plt.show() # Show the plot if not saving\n",
    "\n",
    "print(\"\\nComparison plotting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff70b9-b41f-472e-88ca-5bd60ff15975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
